<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <title>Information Theory</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
  <link rel="stylesheet" href="../../styles/slides.css">
  <script src="../../js/ix.js"></script>
  <script data-main="../../js/slides.js" src="../../third-js/requirejs/require.js"></script>
  <style>
    .crypt {
      font-size: 1em !important;
    }
  </style>
</head>
<body>
<div class="reveal">
  <div class="slides">
    <section class="cover">
      <h1>measuring information</h1>
      <h2>info-20002: foundations of informatics</h2>
    </section>
    <section data-markdown>
      <script type="text/template">
information theory
=================

- Shannon's seminal paper
- Provide a framework to deal with the efficiency of information transmission in noisy channels
- Measure information quantity objectively
- Datt as facts
- Devoid of the subjective aspects of information, no semantics and pragmatics.
- Information represents **the degree of freedom in choosing one particular symbol from all possible ones.**
- Bit as the fundamental unit of information.

<div class="footer">
  <div>[1] Shannon, C. (1948). <a href="http://plan9.bell-labs.com/cm/ms/what/shannonday/shannon1948.pdf">A Mathematical Theory of Communication</a>, Bell System Technical Journal (27)</div>
  <div>Klir, G. J., Wierman, M. J. (1999). <a href="http://link.springer.com.ezp.lib.unimelb.edu.au/book/10.1007/978-3-7908-1869-7">"Uncertainty-Based Information. Elements of Generalized Information Theory"</a></div>
</div>

      </script>
    </section>

    <section data-markdown>
      <script type="text/template">
Binary Code
===========

- Two-symbol language: two alphabets "0" and "1", binary digits
- Bit is equivalent to the selection between two equally likely choices
- Information quantity depends on the number of alternative message choices encoded in the binary system
- Most economical information encoding (von Baeyer, 2004)
      </script>
    </section>

    <section data-markdown>
      <script type="text/template">
Consider encoding a number between 0 and 127 inclusive using a set of numbered flags.
Number 126 can be represented using various encodings as follows:
        
| Method        | Required flags           | Encoded number  |
| ------------- |-------------|-----|
| One number each flag | 128 | 126 |
| Hexadecimal code      | 23 = \\(7 + (1 \\times 16)\\) |   7E |
| Decimal code      | 21 = \\(1 + (2 \\times 10)\\) |   126 |
| Octal code      | 17 = \\(1 + (2 \\times 8)\\) |   176 |
| Binary code | 14 = \\(2 \\times 7\\)  |  11111110 |
      </script>
    </section>

    <section>
<h1>Measure of Information</h1>

<h2>Hartley function (Hartley uncertainty measure)</h2>
<ul>
<li>Selecting up a particular symbol on a finite set \(X\) of uniformly discrete symbols.</li>
<li>The amount of information needed to remove the uncertainty.</li>
<li>The amount of uncertainty associated with a set of \(n\) alternatives.</li>
<li>Quantifies how many \(n\)-choice questions need to be asked to uncover the selected symbol<br/>
  <!--(for binary digit the questions would be yes/no questions)--></li>
</ul>
\[
\begin{aligned}
H_0(X)=\log_b \left | X\right |
\end{aligned}
\]
<p>
\(b = 2\), the unit of measurement is called <strong>shannon</strong> (Sh).<br />
\(b = e\), the unit of measurement is called <strong>nat</strong> (for natural logarithmic).<br />
\(b = 10\), the unit of measurement is called <strong>hartley</strong> (Hart).
</p>
        <div class="footer">
          <div>[1] Hartley, R. V. L. (1928). <a href="http://dx.doi.org.ezp.lib.unimelb.edu.au/10.1002/j.1538-7305.1928.tb01236.x">"Transmission of information,"</a>, Bell System Technical Journal 7(3): 535-563</div>
        </div>
    </section>

    <section>
        <iframe style="overflow:hidden;width:800px;height:600px" src="../introduction/reader/reader.html"></iframe>
    </section>

    <section>
      <h1>Entropy of Information</h1>

      <ul>
        <li>Shannon's Entropy probabilistic-based measurement</li>
        <li>Information as events (random variable), e.g. tossing coins, receiving a piece of text</li>
        <li>Hartley works for uniformly distributed symbol, Shannon's for non-uniform distribution</li>
        <li>Measure of the amount of uncertainty in selecting a particular symbol from symbol space</li>
        <li>The more unpredictable a symbol is, the higher its information value</li>        
      </ul>
      <div class="footer">
        <div>[1] Hartley, R. V. L. (1928). <a href="http://dx.doi.org.ezp.lib.unimelb.edu.au/10.1002/j.1538-7305.1928.tb01236.x">"Transmission of information,"</a>, Bell System Technical Journal 7(3): 535-563</div>
      </div>
    </section>

    <section>
      <h2>Entropy of Information</h2>

      <p>Let:</p>
      <ul>
        <li>a message space of \(n\) symbols in a possible space \(X\).
          Probablistically speaking, \(X\) is a discrete random variable with \(n\) outcomes
          \(\left \{ x_1, ..., x_n \right \}\).
        </li>
        <li>\(p(x_i)\) is the probability of selecting \(x_i\)</li>
        <li>measure of information content (self-information or surprisal) of symbol \(x_i\) is \(-\log p(x_i)\)</li>
        <li>Entropy of selecting a symbol from \(X\) is:</li>
      </ul>
      \[
      \begin{aligned}
      H(X)=- \sum_{i=1}^{n} p(x_i) \log p(x_i)
      \end{aligned}
      \]
      <div class="footer">
        <div>[1] Hartley, R. V. L. (1928). <a href="http://dx.doi.org.ezp.lib.unimelb.edu.au/10.1002/j.1538-7305.1928.tb01236.x">"Transmission of information,"</a>, Bell System Technical Journal 7(3): 535-563</div>
      </div>
    </section>
    
    <section>
      <h2>Entropy of a Binary Message</h2>

      <p>Consider a message, Head or Tail, constructed from the result of tossing a coin
      </p>
      <ul style="font-size:0.7em; line-height: 2em !important;">
        <li>Case 1: Only Heads<br />
          \(H = -(1 \log_2 1 + 0 \log_2 0) = 0\) bit</li>
        <li>Case 2: Biased coin, 80% Heads and 20% Tails<br /> 
          \(H = -(0.8 \log_2 0.8 + 0.2 \log_2 0.2) = 1.85\) bit</li>
        <li>Case 3: Fair Coin<br />
          \(H = -(2 \times 0.5 \log_2 0.5 = 1\) bit</li>
      </ul>

      <table id="fig-entropy-coin">
        <tr>
          <td class="first"></td>
          <td></td>
          <td></td>
          <td></td>
        </tr>
      </table>
      <script>
        ix.register(function () {
          var charts = [],
              containers = $('#fig-entropy-coin td'),
              datasets = [
                {
                  name: 'Case 1: H = 0',
                  data: [1, 0]
                  },
                {
                  name: 'Case 2: H = 0.72',
                  data: [0.8, 0.2]
                },
                {
                  name: 'Case 3: H = 1',
                  data: [0.5, 0.5]
                }
              ];

          $.each(datasets, function(i, dataset) {
            var offset = (i === 0) ? 50 : 20;
            charts.push(new Highcharts.Chart({
              chart: {
                width: 200 + offset + (!i ? -20: 0),
                height: 200,
                renderTo: containers[i],
                type: 'bar',
                marginLeft: offset
              },
              title: {
                text: dataset.name,
                align: 'left',
                x: offset
              },
              xAxis: {
                categories: ['Tail', 'Head'],
                labels: { enabled: i === 0 }
              },

              yAxis: {
                allowDecimals: true,
                lineWidth: 1,
                tickInterval: 0.1,
                title: {
                  text: null
                },
                min: 0,
                max: 1
              },
              credits: { enabled: false },
              tooltip: { enabled: false },
              legend: { enabled: false},
              series: [dataset]
            }));
          });

        });
      </script>
    </section>
    
    <section>
      <h2>Properties of Entropy</h2>
      <div id="binary-entropy" class="figure" style="width:50%"></div>
      <script>
        ix.register(function () {
          var n = 20;
          var a = _.map(_.range(n+1), function(i) { p = i/n; h = -p * Math.log2(p) - (1-p) * Math.log2(1-p); return [i/n, isNaN(h) ? 0 : h ];});
          var data = _.reduce(a, function (m,n) { return m.concat([n]); }, [])
          
        $('#binary-entropy').highcharts({
        chart: {
            type: 'spline'
        },
        title: {
            text: 'Binary Entropy Function'
        },
        xAxis: {
          title: {
              text: 'p(x=Head)'
          }
        },
        yAxis: {
            title: {
                text: 'H(X)'
            },
                min: 0,
                max: 1            
        },
        plotOptions: {
            spline: {
                lineWidth: 4,
                marker: {
                    enabled: false
                }
            }
          },        
        series: [{
            data: data
        }],
        tooltip: { enabled: false },
        legend: { enabled: false},
      });
    });
      </script>
      <ul class="smaller">
        <li>\(H\) is maximum when all \(p(x_i)\) are equal.</li>
        <li>\(H\) is minimum when any of \(p(x_i)\) is 1.</li>
      </ul>
    </section>

    <section>
      <h2>Example with 4 symbols</h2>


      <p>Consider a message space of A, T, G, C characters with various probability distributions.
      </p>
      <ul style="font-size:0.7em; line-height: 2em !important;">
        <li>Case 1: \(H = -(1 \log_2 1 + 3 \times 0 \log_2 0) = 0\)</li>
        <li>Case 2: \(H = -(0.4 \log_2 0.4 + 0.3 \log_2 0.3 + 0.2 \log_2 0.2 + 0.1 \log_2 0.1) = 1.85\)</li>
        <li>Case 3: \(H = -(4 \times \frac{1}{4} \log_2 \frac{1}{4}) = 2\)</li>
      </ul>
      <table class="figure" id="fig-entropy">
        <tr>
          <td class="first"></td>
          <td></td>
          <td></td>
          <td></td>
        </tr>
      </table>
      <script>
        ix.register(function () {
          var charts = [],
              containers = $('#fig-entropy td'),
              datasets = [
                {
                  name: 'Case 1: H = 0',
                  data: [1, 0, 0, 0]
                },
                {
                  name: 'Case 2: H = 1.85',
                  data: [0.4, 0.3, 0.2, 0.1]
                },
                {
                  name: 'Case 3: H = 2',
                  data: [0.25, 0.25, 0.25, 0.25]
                }
              ];

          $.each(datasets, function(i, dataset) {
            var offset = (i === 0) ? 50 : 20;
            charts.push(new Highcharts.Chart({
              chart: {
                width: 200 + offset + (!i ? -20: 0),
                height: 200,
                renderTo: containers[i],
                type: 'bar',
                marginLeft: offset
              },
              title: {
                text: dataset.name,
                align: 'left',
                x: offset
              },
              xAxis: {
                categories: ['A', 'T', 'G', 'C'],
                labels: { enabled: i === 0 }
              },

              yAxis: {
                allowDecimals: true,
                lineWidth: 1,
                tickInterval: 0.1,
                title: {
                  text: null
                },
                min: 0,
                max: 1
              },
              credits: { enabled: false },
              tooltip: { enabled: false },
              legend: { enabled: false},
              series: [dataset]
            }));
          });

        });
      </script>
    </section>

    <section data-markdown>
      <script type="text/template">
\\(H\\) indicates the theoretical bounds for the average bits needed to represent/compress the symbols. Consider a message constructed from `A`, `T`, `G`, `C` with distibution of \\([0.4, 0.3, 0.2, 0.1]\\)

- Binary encode `A` as `00`, `T` as `01`, `G` as `10`, and `C` as `11` (fixed-length)
- Average message length = <br />\\((0.4 \\times 2 + 0.3 \\times 2 + 0.2 \\times 2 + 0.1 \\times 2) = 2\\)

Alternatively:

- Binary encode `A` as `0`, `T` as `10`, `G` as `110`, and `C` as `111` (variable-length)
- Average message length = <br />\\((0.4 \\times 1 + 0.3 \\times 2 + 0.2 \\times 3 + 0.1 \\times 3) = 1.9 > H = 1.85\\)
- Using variable-length encoding (in this case [Huffman coding](http://mathworld.wolfram.com/HuffmanCoding.html)), the size is 5% smaller
      </script>
    </section>

  </div>
</div>
</body>
</html>